{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"zero_mean_unit_variance.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hlTJF2Cc4cD2","colab_type":"text"},"source":["## Zero Mean and Unit Variance\n","<br>Scaling data to zero mean and unit variance is a example of normalization in here are several advantages of normalization, many of which are interrelated:</br>\n","<br>Makes training less sensitive to the scale of features: Consider a regression problem where you’re given features of an apartment and are required to predict the price of the apartment. Let’s say there are 2 features — no. of bedrooms and the area of the apartment. Now, the no. of bedrooms will be in the range 1–4 typically, while the area will be in the range 100–200𝑚2. If you’re modelling the task as linear regression, you want to solve for coefficients 𝑤1and 𝑤2 corresponding to no. of bedrooms and area. Now, because of the scale of the features, a small change in 𝑤2 will change the prediction by a lot compared to the same change in 𝑤1, to the point that setting 𝑤2 correctly might dominate the optimization process.</br>\n","<br>Regularization behaves differently for different scaling: Suppose you have an ℓ2regularization on the problem above. It is easy to see that ℓ2 regularization pushes larger weights towards zero more strongly than smaller weights. So consider that you obtain some optimal values of 𝑤1and 𝑤2 using your given unnormalized data matrix 𝑋. Now instead of using 𝑚2 as the unit of area, if I change the data to represent area in 𝑓𝑡2, the corresponding column of X will get multiplied by a factor of ~10. Therefore, you would expect the corresponding optimal coefficient 𝑤2 to go down by a factor of 10 to maintain the value of y. But, as stated before, the ℓ2 regularization now has a smaller effect because of the smaller value of the coefficient. So you will end up getting a larger value of 𝑤2 than you would have expected. This does not make sense — you did not change the information content of the data, and therefore, your optimal coefficients should not have changed.</br>\n","<br>Consistency for comparing results across models: As covered in point 2, scaling of features affects performance. So, if there are scientists developing new methods, and compare previous state-of-the-art methods with their new methods, which uses more carefully chosen scaling, then the results will not be reliable.</br>\n","<br>Makes optimization well-conditioned: Most machine learning optimizations are solved using gradient descent, or a variant thereof. And the speed of convergence depends on the scaling of features (or more precisely, the eigenvalues of 𝑋𝑇𝑋, which explains why zero mean helps). Normalization makes the problem better conditioned, improving the convergence rate of gradient descent. I give an intuition of this using a simple example below.</br>\n","<br>Consider the simplest case where 𝐴 is a 2 x 2 diagonal matrix, say 𝐴=𝑑𝑖𝑎𝑔([𝑎1,𝑎2]). Then, the contours of the objective function ‖𝐴𝑥−𝑏‖2 will be axis-aligned ellipses as shown in the figure below:\n","\n","\n","Suppose you start at the point marked in red. Observe that to reach the optimal point, you need to take a very large step in the horizontal direction but a small step in the vertical direction. The descent direction is given by the green arrow. If you go along this direction, then you will move larger distance in the vertical direction and smaller distance in the horizontal direction, which is the opposite of what you want to do!\n","\n","If you take a small step along the gradient, covering the large horizontal distance to the optimal is going to take a large number of steps. If you take a large step along the gradient, you will overshoot the optimal in the vertical direction.\n","\n","This behavior is due to the shape of the contours. The more circular the contours are, the faster you will converge to the optimal. The elongation of the ellipses is given by the ratio of the largest and the smallest eigenvalues of the matrix 𝐴\n","A\n",". In general, the convergence of an optimization problem is measured by its condition number, which in this case is the ratio of the two extreme eigenvalues.\n","\n","(Prasoon Goyal's answer to Why is the Speed Of Convergence of gradient descent depends on the maximal and minimal eigenvalues of A in solving AX=b through least squares.)\n","\n","Finally, I should mention that normalization does not always help, as far as accuracy is concerned. Here's a simple example : consider a problem with only one feature with variance 1. Now suppose I add a dummy feature with variance 0.01. If you regularize your model correctly, the solution will not change much because of this dummy dimension. But if you now normalize it to have unit variance, it might hurt the performance. </br>"]},{"cell_type":"markdown","metadata":{"id":"JW910YplBl3B","colab_type":"text"},"source":["### Notes on standard deviation and variance: \n","![normal_distribution](https://upload.wikimedia.org/wikipedia/commons/8/8c/Standard_deviation_diagram.svg)\n","<br>In statistics, the standard deviation (SD, also represented by the lower case Greek letter sigma σ) is a measure that is used to quantify the amount of variation or spread of a set of data values.</br>\n","<br>A low standard deviation indicates that the data points tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the data points are spread out over a wider range of values.</br>"]},{"cell_type":"code","metadata":{"id":"C6aifr5E4XSe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"outputId":"a5ce97c6-20b4-4be9-a176-32cb65d7ca75","executionInfo":{"status":"ok","timestamp":1560735150409,"user_tz":180,"elapsed":376,"user":{"displayName":"tianye wang","photoUrl":"https://lh6.googleusercontent.com/-Mx2x3Dd5gPA/AAAAAAAAAAI/AAAAAAAAAOQ/sqYzqu602T8/s64/photo.jpg","userId":"12454609316683315269"}}},"source":["import numpy as np\n","\n","data = np.random.rand(10,2)*10\n","\n","print(data)\n","print(\"mean:\",np.mean(data, axis = 0))\n","print(\"variance:\",np.var(data, axis = 0))"],"execution_count":64,"outputs":[{"output_type":"stream","text":["[[9.13035522 2.95174588]\n"," [0.74890874 3.66603453]\n"," [1.93508603 8.93285109]\n"," [4.4791619  7.18034272]\n"," [1.57212201 1.84023272]\n"," [5.8401751  4.65987914]\n"," [0.21427841 7.80071103]\n"," [8.00363362 6.49762062]\n"," [7.26773536 6.87702282]\n"," [7.47464145 7.23799235]]\n","mean: [4.66660978 5.76444329]\n","variance: [9.93327524 4.90707711]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1rgnw3G38Wcd","colab_type":"text"},"source":["### One way of doing it: sklearn package"]},{"cell_type":"code","metadata":{"id":"lMDYVb3d7S0Y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"outputId":"af46885d-1736-47cc-b218-482a99155556","executionInfo":{"status":"ok","timestamp":1560735151479,"user_tz":180,"elapsed":294,"user":{"displayName":"tianye wang","photoUrl":"https://lh6.googleusercontent.com/-Mx2x3Dd5gPA/AAAAAAAAAAI/AAAAAAAAAOQ/sqYzqu602T8/s64/photo.jpg","userId":"12454609316683315269"}}},"source":["from sklearn import preprocessing\n","\n","data_scaled = preprocessing.scale(data)\n","\n","print(data_scaled)\n","print(\"scaled mean:\",data_scaled.mean(axis = 0))\n","print(\"variance:\",data_scaled.var(axis = 0))"],"execution_count":65,"outputs":[{"output_type":"stream","text":["[[ 1.41629325 -1.26973056]\n"," [-1.24303987 -0.94728061]\n"," [-0.86667994  1.43030821]\n"," [-0.05947498  0.63917674]\n"," [-0.9818441  -1.7714988 ]\n"," [ 0.37235829 -0.49863126]\n"," [-1.4126717   0.91922841]\n"," [ 1.05879791  0.33097683]\n"," [ 0.82530616  0.50224963]\n"," [ 0.89095499  0.66520141]]\n","scaled mean: [-3.44169138e-16 -1.88737914e-16]\n","variance: [1. 1.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i18aiJB9-1n_","colab_type":"text"},"source":["### Another way: Impliment it!\n","Note: standard deviation equales the square root of variance"]},{"cell_type":"code","metadata":{"id":"1756DWpX-Hm3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"outputId":"8186bc52-55b1-4c1e-e2e5-fb34b871550b","executionInfo":{"status":"ok","timestamp":1560735152629,"user_tz":180,"elapsed":300,"user":{"displayName":"tianye wang","photoUrl":"https://lh6.googleusercontent.com/-Mx2x3Dd5gPA/AAAAAAAAAAI/AAAAAAAAAOQ/sqYzqu602T8/s64/photo.jpg","userId":"12454609316683315269"}}},"source":["scaled_data = (data - data.mean(axis=0)) / data.std(axis=0)\n","\n","print(scaled_data)\n","print(\"scaled mean:\",scaled_data.mean(axis = 0))\n","print(\"variance:\",scaled_data.var(axis = 0))"],"execution_count":66,"outputs":[{"output_type":"stream","text":["[[ 1.41629325 -1.26973056]\n"," [-1.24303987 -0.94728061]\n"," [-0.86667994  1.43030821]\n"," [-0.05947498  0.63917674]\n"," [-0.9818441  -1.7714988 ]\n"," [ 0.37235829 -0.49863126]\n"," [-1.4126717   0.91922841]\n"," [ 1.05879791  0.33097683]\n"," [ 0.82530616  0.50224963]\n"," [ 0.89095499  0.66520141]]\n","scaled mean: [-3.44169138e-16 -1.88737914e-16]\n","variance: [1. 1.]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oLXJQMa7EHh0","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}