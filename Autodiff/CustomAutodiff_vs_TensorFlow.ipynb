{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CustomAutodiff_vs_TensorFlow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy4pJPTIKDEH",
        "colab_type": "text"
      },
      "source": [
        "# <center>Custom Autodiff</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM3C2NbTKDEI",
        "colab_type": "text"
      },
      "source": [
        "In this project, you will learn to build a reasonably self-sufficient Custom AutoDiff and test the same by comparing your output against an identical Feed-forward neural network implemented using Tensorflow. \n",
        "Later, you will perform a set of experiments to get a better feel of:\n",
        "- the difficulties in optimizing certain Neural Network Architectures. \n",
        "- solutions for improving the performance. \n",
        "- the sensitivity to learning-rate in certain Architectures, etc. \n",
        "\n",
        "We will use the MNIST data for this project\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqiK1_3KKDEJ",
        "colab_type": "code",
        "outputId": "4238d76f-c62d-4a77-9b08-26b3ff66253b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import random \n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.14.0-rc1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdaABfINKDEP",
        "colab_type": "code",
        "outputId": "4b187db7-9e59-4287-b7ad-d999a4413455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "source": [
        "# Load MNIST Data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "train_data = mnist.train.images # Returns np.array\n",
        "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
        "eval_data = mnist.test.images # Returns np.array\n",
        "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
        "print(train_data.shape)\n",
        "print(train_labels.shape)\n",
        "print(eval_data.shape)\n",
        "print(eval_labels.shape)\n",
        "# Randomly choose 10 images from first 50 images of Train Data.\n",
        "for index,idx in enumerate(random.sample(range(50),10)): \n",
        "    plt.subplot(10,1,index+1)\n",
        "    plt.imshow(train_data[idx].reshape(28,28))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-cdc8e2063063>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "(55000, 784)\n",
            "(55000, 10)\n",
            "(10000, 784)\n",
            "(10000, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADkAAAD8CAYAAADNPQyCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyf8A2ihoJpC47R8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5TYQ65CKDES",
        "colab_type": "text"
      },
      "source": [
        "## Custom AutoDiff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV1Bi8LzKDET",
        "colab_type": "text"
      },
      "source": [
        "The code for the Custom AutoDiff is in the file CustomAutoDiff.py.\n",
        "\n",
        "The following is the quick summary of the features and limitations:\n",
        "- Every node in the graph is considered a Variable. This means that the gradient will be computed with respect to \"constants\" (Ex: inputs,outputs,etc) even though we do not need the gradients. \n",
        "- Every Variable should be represented as a 2D array. A scalar is automatically converted to a 2D array as required. \n",
        "- The following lists the set of supported operations and their syntax:\n",
        "    * $e^\\mathbf{x}$ => `x.exp()`\n",
        "    * $log(\\mathbf{x})$ => `x.log()`\n",
        "    * Matrix-Addition (allows broadcasting) => `x + y`\n",
        "    * Elementwise-Multiplication (allows broadcasting) => `x * y`\n",
        "    * Elementwise-Division (allows broadcasting) => `x/y`\n",
        "    * Matrix-Multiplication => `x @ y`\n",
        "    * Sum along the dimension D => `x.reduce_sum(axis=D)`\n",
        "        - Allows only 1 axis at a time.\n",
        "        - All operations retain the rank of x.\n",
        "    * Activation Functions (Sigmoid, Tanh, ReLU): `ReLU(x)`\n",
        "- Automatically converts a numpy object/list/scalar into a `Variable`. However, the first operand of the operation should be a `Variable`. If `x = Variable([[1,2,3]])`:\n",
        "    * `x+10` is valid.\n",
        "    * `10+x` is not! (Why?) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R71E8D23KDET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# The autoreload helps in automatically reloading the module as soon as \n",
        "# it changes on the disk. \n",
        "import CustomAutoDiff as cad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQVs_V_y9sJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFffSdsDKDEW",
        "colab_type": "code",
        "outputId": "0a79389b-528d-4191-e76e-73eece79d872",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "def test():\n",
        "    \"\"\"\n",
        "    Tests MatrixMultiplication.\n",
        "    \"\"\"\n",
        "    v1 = cad.Variable([[1,2],\n",
        "                       [3,4]])\n",
        "    v2 = cad.Variable([[1],\n",
        "                       [3]])\n",
        "    \n",
        "    mult_ = v1 @ v2\n",
        "    print(\"MatMul\")\n",
        "    print(mult_.value)\n",
        "    \n",
        "    mult_.gradient = np.array([[1],\n",
        "                               [3]])\n",
        "    \n",
        "    mult_.backward()\n",
        "    \n",
        "    print(\"Derivatives\")\n",
        "    print(v1.gradient)\n",
        "    print(v2.gradient)\n",
        "test()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MatMul\n",
            "[[ 7.]\n",
            " [15.]]\n",
            "Derivatives\n",
            "[[1. 3.]\n",
            " [3. 9.]]\n",
            "[[10.]\n",
            " [14.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLnYsLsYKDEZ",
        "colab_type": "code",
        "outputId": "2204ea6f-4b5b-445e-efe8-ae79a718c592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "def test():\n",
        "    \"\"\"\n",
        "    Tests MatrixAddition (Broadcasting).\n",
        "    Check by transposing v2 as well.\n",
        "    \"\"\"\n",
        "    v1 = cad.Variable([[1,2],\n",
        "                       [3,4]])\n",
        "    v2 = cad.Variable([[1],\n",
        "                       [3]])\n",
        "    # Repeat the test by uncommenting the next line.\n",
        "    #v2.value = v2.value.T \n",
        "    sum_ = v1 + v2\n",
        "    print(\"Sum\")\n",
        "    print(sum_.value)\n",
        "    \n",
        "    sum_.gradient = np.array([[1,0],\n",
        "                              [0,3]])\n",
        "    \n",
        "    sum_.backward()\n",
        "    print(\"Derivatives\")\n",
        "    print(v1.gradient)\n",
        "    print(v2.gradient)    \n",
        "test()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sum\n",
            "[[2. 3.]\n",
            " [6. 7.]]\n",
            "Derivatives\n",
            "[[1 0]\n",
            " [0 3]]\n",
            "[[1]\n",
            " [3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oeU9vvgKDEb",
        "colab_type": "code",
        "outputId": "ad7afd1c-9a71-4a9d-a467-3f7512d189cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "def test():\n",
        "    \"\"\"\n",
        "    Tests reduce_sum, log, exp, MatrixDivision and ElementwiseMultiplication.\n",
        "    \"\"\"\n",
        "    v = cad.Variable([[1,2,3],\n",
        "                      [3,10,5]])\n",
        "    sm = v.exp()/v.exp().reduce_sum(axis=1)\n",
        "    print(\"Softmax output\")\n",
        "    print(sm.value)\n",
        "    \n",
        "    loss = (sm.log()*([[0,1,0],\n",
        "                      [1,0,0]])).reduce_sum(axis=0).reduce_sum(axis=1)\n",
        "    print(\"Loss\")\n",
        "    print(loss.value)\n",
        "    \n",
        "    loss.gradient = 1\n",
        "    loss.backward()\n",
        "    \n",
        "    print(\"Derivative wrt softmax\")\n",
        "    print(sm.gradient)\n",
        "    \n",
        "    print(\"Derivative wrt logits\")\n",
        "    print(v.gradient)\n",
        "test()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Softmax output\n",
            "[[9.00305732e-02 2.44728471e-01 6.65240956e-01]\n",
            " [9.04959183e-04 9.92408247e-01 6.68679417e-03]]\n",
            "Loss\n",
            "[[-8.41522668]]\n",
            "Derivative wrt softmax\n",
            "[[   0.            4.08616127    0.        ]\n",
            " [1105.02221453    0.            0.        ]]\n",
            "Derivative wrt logits\n",
            "[[-0.09003057  0.75527153 -0.66524096]\n",
            " [ 0.99909504 -0.99240825 -0.00668679]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQWhM4daKDEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activation_map = {\n",
        "    \"sigmoid\": [cad.Sigmoid,tf.sigmoid],\n",
        "    \"relu\": [cad.ReLU,tf.nn.relu],\n",
        "    \"tanh\": [cad.Tanh,tf.tanh]\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfLEatqdKDEh",
        "colab_type": "text"
      },
      "source": [
        "## Custom NN\n",
        "\n",
        "Already done for you, answer the questions in the comments. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3yIZXdnKDEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomNN:\n",
        "    def __init__(self,conf):\n",
        "        self.weights = [cad.Variable(weight) for weight in conf.weights]\n",
        "        self.biases = [cad.Variable(bias) for bias in conf.biases]\n",
        "        self.activations = [activation_map[activation][0] for activation in conf.activations]\n",
        "        self.learning_rate = conf.learning_rate\n",
        "        self.mu = conf.momentum\n",
        "        \n",
        "    def forward(self,batch):\n",
        "        # Describe what the following function does. \n",
        "        '''\n",
        "        The function use for loop to iterate each weight, bias and activations that are zip together.\n",
        "        it also define the logits and softmax function when foward pass. \n",
        "        This is similar to building the computation graph in Tensorflow.\n",
        "        '''\n",
        "        temp = cad.Variable(batch)\n",
        "        for weight,bias,act in zip(self.weights,self.biases,self.activations):\n",
        "            temp = temp@weight+bias\n",
        "            temp = act(temp)\n",
        "        logits = temp@self.weights[-1]+self.biases[-1]\n",
        "        softmax = logits.exp()/logits.exp().reduce_sum(axis=1)\n",
        "        return logits.value\n",
        "    \n",
        "    def loss_and_gradients(self,batch,targets):\n",
        "        for param in self.weights+self.biases:\n",
        "            param.reset()\n",
        "            \n",
        "        temp = cad.Variable(batch)\n",
        "        for weight,bias,act in zip(self.weights,self.biases,self.activations):\n",
        "            temp = temp@weight+bias\n",
        "            temp = act(temp)\n",
        "        \n",
        "        logits = temp@self.weights[-1]+self.biases[-1]\n",
        "        \n",
        "        # Why do we do this? \n",
        "        '''\n",
        "        The reason we do this is to stable network in an numerical way. \n",
        "        In softmax the exponential function increases very fast, \n",
        "        to increase the stability we can subtract the maximum value withou make change to the softmax result. \n",
        "        '''\n",
        "        logits = logits + (-logits.value.max(axis=1,keepdims=True))\n",
        "        softmax = logits.exp()/logits.exp().reduce_sum(axis=1)\n",
        "        \n",
        "        # Why did we add 10**-6 to softmax?\n",
        "        '''\n",
        "        We add this is because we want to control the percision of the results\n",
        "        '''\n",
        "        # Why did we multiply with \"-targets\"?\n",
        "        '''\n",
        "        This looks like the Cross Entropy loss to me, the functiong is defined like this, \n",
        "        so that  the reason we multiply with negative target.\n",
        "        or in an neumerical way we could say we want our loss to be positive \n",
        "        softmax is a decimal number between 0 and 1 the log of it would be negative,\n",
        "        if we want to get positive loos we multiply a negative number which is the negative of target. \n",
        "        '''\n",
        "        loss = ((softmax+10**-6).log() * cad.Variable(-targets)).reduce_sum(axis=0).reduce_sum(axis=1)/batch.shape[0]\n",
        "        \n",
        "        loss.gradient = 1\n",
        "        loss.backward()\n",
        "        return {\n",
        "            \"loss\" :loss.value,\n",
        "            \"grads\":[param.gradient for param in (self.weights+self.biases)]\n",
        "        }\n",
        "    \n",
        "    def apply_gradients(self):\n",
        "        #initial velocity is zero\n",
        "        v = 0\n",
        "        for param in self.weights+self.biases:\n",
        "            param.value -= self.learning_rate * param.gradient\n",
        "            \n",
        "            param.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_9gkKh_KDEk",
        "colab_type": "text"
      },
      "source": [
        "## Tensorflow NN\n",
        "\n",
        "Already done for you, answer the questions in the comments. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MVG-hg8KDEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TensorflowNN:\n",
        "    def __init__(self,conf):\n",
        "        # Why is it a good idea to build the compute graph within the tf.Graph scope?\n",
        "        '''\n",
        "        1: Tensorflow's session will caches information about the graph\n",
        "        2: For parallesims, differetn part of the graph can run in parallel to save computation time. \n",
        "        '''\n",
        "        g = tf.Graph()\n",
        "        with g.as_default():\n",
        "            # Define the placeholders\n",
        "            self.input = tf.placeholder(dtype=tf.float64, shape=(None, 784))\n",
        "            self.expected_output = tf.placeholder(dtype=tf.float64, shape=(None, 10))\n",
        "            \n",
        "            self.weights = [tf.Variable(dtype=tf.float64,initial_value=weight) for weight in conf.weights]\n",
        "            self.biases = [tf.Variable(dtype=tf.float64,initial_value=bias) for bias in conf.biases]\n",
        "\n",
        "            # Build the graph for computing output.\n",
        "            layer = self.input\n",
        "            for i in range(0, len(conf.activations)):\n",
        "                layer = layer@self.weights[i] + self.biases[i]\n",
        "                layer = activation_map[conf.activations[i]][1](layer)\n",
        "            \n",
        "            # For output layer\n",
        "            self.logits = layer@self.weights[-1] + self.biases[-1]\n",
        "            logits = self.logits - tf.reduce_max(self.logits,axis=1,keepdims=True)\n",
        "            \n",
        "            self.output = tf.exp(logits)/tf.reduce_sum(tf.exp(logits),axis=1,keepdims=True)\n",
        "            self.loss = -tf.reduce_mean(tf.reduce_sum(tf.log(self.output+10**-6)*self.expected_output,axis=1))\n",
        "            \n",
        "            # Instantiate the optimizer\n",
        "            #self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=conf.learning_rate)\n",
        "            self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=conf.learning_rate)\n",
        "            \n",
        "            self.grads = self.optimizer.compute_gradients(self.loss)\n",
        "            self.train_op = self.optimizer.apply_gradients(self.grads)\n",
        "            \n",
        "            self.session = tf.Session()\n",
        "            \n",
        "            # Initialize all variables\n",
        "            self.session.run(tf.global_variables_initializer())\n",
        "    \n",
        "    def forward(self,batch):\n",
        "        return self.session.run(self.logits,feed_dict={self.input:batch})\n",
        "    \n",
        "    def loss_and_gradients(self,batch,targets):\n",
        "        return {\n",
        "            \"loss\" :self.session.run(self.loss,feed_dict={self.input:batch,self.expected_output:targets}),\n",
        "            \"grads\":self.session.run(tf.gradients(self.loss,self.weights+self.biases),feed_dict={self.input:batch,self.expected_output:targets})\n",
        "        }\n",
        "    \n",
        "    def apply_gradients(self,batch,targets):\n",
        "        self.session.run(self.train_op,feed_dict={self.input:batch,self.expected_output:targets})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTWKti4GKDEo",
        "colab_type": "text"
      },
      "source": [
        "## Comparing Custom NN and Tensorflow NN\n",
        "\n",
        "Just run the following code snippets after filling out CustomAutoDiff.py."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRNb1eraKDEp",
        "colab_type": "text"
      },
      "source": [
        "Preparing weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZx3_DPMKDEp",
        "colab_type": "code",
        "outputId": "60e16866-4d10-49d4-9bd2-128ae144800f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "class Conf:\n",
        "    def __init__(self,hidden_layers,activations,learning_rate,momentum):\n",
        "        number_units = [784] + hidden_layers + [10]\n",
        "        weights = []\n",
        "        biases = []\n",
        "        for prev,curr in zip(number_units,number_units[1:]):\n",
        "            np.random.seed(1)\n",
        "            weights.append(np.random.randn(prev,curr))\n",
        "            biases.append(np.zeros([1,curr],dtype=np.float32))\n",
        "        self.weights = weights\n",
        "        self.biases = biases\n",
        "        self.activations = activations\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "conf = Conf(hidden_layers=[300],\n",
        "           activations=[\"sigmoid\"],\n",
        "           learning_rate=0.1)\n",
        "\"\"\"\n",
        "Let's create two instances of the above Neural Network configuration - \n",
        "one built on custom AutoDiff framework and the other built on Tensorflow Library. \n",
        "\"\"\"\n",
        "\n",
        "customNN = CustomNN(conf)\n",
        "tensorNN = TensorflowNN(conf)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uILfIyVKDEr",
        "colab_type": "text"
      },
      "source": [
        "We will first make sure that our code is correct by comparing the results against that of Tensorflow's. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgXFK1X3KDEs",
        "colab_type": "text"
      },
      "source": [
        "<b>Comparing forward pass</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voEZotgsKDEt",
        "colab_type": "code",
        "outputId": "b20317ee-8fc3-48b2-e346-3328ade84445",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "idx = random.choice(range(len(train_data)))\n",
        "t1 = customNN.forward(train_data[idx:idx+5])\n",
        "t2 = tensorNN.forward(train_data[idx:idx+5])\n",
        "print((np.abs(t2-t1)<10**-6).all())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxNx-0wAKDEw",
        "colab_type": "text"
      },
      "source": [
        "<b>Comparing Gradients</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "v9mSZbbUKDEw",
        "colab_type": "code",
        "outputId": "1f11dbff-5018-414e-952a-505a6348449c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "\n",
        "idx = random.choice(range(train_data.shape[0]))\n",
        "t1 = customNN.loss_and_gradients(train_data[idx:idx+5],train_labels[idx:idx+5])\n",
        "t2 = tensorNN.loss_and_gradients(train_data[idx:idx+5],train_labels[idx:idx+5])\n",
        "print(t1[\"loss\"],t2[\"loss\"])\n",
        "for i,j in zip(t1[\"grads\"],t2[\"grads\"]):\n",
        "   \n",
        "    print((np.abs(j-i)<10**-6).all())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[13.59392699]] 13.593926991094104\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6het4ONRKDEz",
        "colab_type": "text"
      },
      "source": [
        "<b>Comparing Test Accuracies for 10 steps of training </b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "-MsQv7B6KDE0",
        "colab_type": "code",
        "outputId": "1ca6fd5f-571f-496a-c301-81cf3e358de0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "for epoch in range(10):\n",
        "    for idx in range(0,train_data.shape[0],128):\n",
        "        # TensorNN update\n",
        "        tensorNN.apply_gradients(train_data[idx:idx+128],train_labels[idx:idx+128])\n",
        "        \n",
        "        # CustomNN update\n",
        "        loss_and_grads1 = customNN.loss_and_gradients(train_data[idx:idx+128],train_labels[idx:idx+128])\n",
        "        customNN.apply_gradients()\n",
        "    \n",
        "    tensorNN_correct = 0\n",
        "    customNN_correct = 0\n",
        "    for idx in range(0,eval_data.shape[0],128):\n",
        "        expected = eval_labels[idx:idx+128]\n",
        "        \n",
        "        # TensorNN Accuracy\n",
        "        preds = tensorNN.forward(eval_data[idx:idx+128])\n",
        "        tensorNN_correct += (np.argmax(preds,axis=1)==np.argmax(expected,axis=1)).sum()\n",
        "        \n",
        "        # CustomNN Accuracy\n",
        "        preds = customNN.forward(eval_data[idx:idx+128])\n",
        "        customNN_correct += (np.argmax(preds,axis=1)==np.argmax(expected,axis=1)).sum()\n",
        "    \n",
        "    print(\"Epoch {}: {} {}\".format(epoch,tensorNN_correct/eval_data.shape[0],customNN_correct/eval_data.shape[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: 0.7526 0.7526\n",
            "Epoch 1: 0.8178 0.8178\n",
            "Epoch 2: 0.8454 0.8454\n",
            "Epoch 3: 0.86 0.86\n",
            "Epoch 4: 0.8702 0.8702\n",
            "Epoch 5: 0.8795 0.8795\n",
            "Epoch 6: 0.8845 0.8845\n",
            "Epoch 7: 0.8894 0.8894\n",
            "Epoch 8: 0.8942 0.8942\n",
            "Epoch 9: 0.8971 0.8971\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXNZBne9KDE6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Now that you have been able to successfully reproduce Tensorflow Results using a custom framework!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpsCQmzByo76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}